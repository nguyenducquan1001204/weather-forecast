import requests
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
import time
import subprocess
import os
from datetime import datetime, timedelta
import sys

if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

def check_and_pull_from_github():
    """S·ª≠ d·ª•ng sync_all.py ƒë·ªÉ pull t·ª´ GitHub"""
    try:
        import sync_all
        return sync_all.check_and_pull()
    except Exception as e:
        print(f"  ‚ö†Ô∏è  Kh√¥ng th·ªÉ import sync_all: {str(e)[:100]}")
        return False

CITY_MAPPING = {
    'ha-noi': 'H√† N·ªôi',
    'vinh': 'Vinh',
    'ho-chi-minh': 'H·ªì Ch√≠ Minh'
}

CITY_URLS = {
    'ha-noi': 'https://thoitiet360.edu.vn/ha-noi/3-ngay-toi',
    'vinh': 'https://thoitiet360.edu.vn/nghe-an/vinh/3-ngay-toi',
    'ho-chi-minh': 'https://thoitiet360.edu.vn/ho-chi-minh/3-ngay-toi'
}

def parse_temperature(temp_str):
    if not temp_str:
        return None
    try:
        temp_str = temp_str.replace('¬∞', '').replace('¬∞C', '').strip()
        return float(temp_str)
    except:
        return None

def parse_pressure(pressure_str):
    if not pressure_str:
        return None
    try:
        pressure_str = pressure_str.replace('hPa', '').strip()
        return float(pressure_str)
    except:
        return None

def parse_wind(wind_str):
    if not wind_str:
        return None
    try:
        wind_str = wind_str.replace('km/h', '').strip()
        return float(wind_str)
    except:
        return None

def parse_rain(rain_str):
    if not rain_str:
        return None
    try:
        rain_str = rain_str.replace('mm', '').strip()
        return float(rain_str)
    except:
        return None

def parse_cloud(cloud_str):
    return None

def crawl_thoitiet360(city_key='ha-noi'):
    url = CITY_URLS.get(city_key)
    if not url:
        print(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y URL cho th√†nh ph·ªë: {city_key}")
        return []
    
    print(f"\nüîç ƒêang crawl d·ªØ li·ªáu t·ª´ thoitiet360.edu.vn cho {CITY_MAPPING.get(city_key, city_key)}...")
    print(f"   URL: {url}")
    
    max_retries = 3
    retry_delay = 2
    
    for attempt in range(max_retries):
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
                'Connection': 'keep-alive'
            }
            
            session = requests.Session()
            response = session.get(url, headers=headers, timeout=30)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                if attempt < max_retries - 1:
                    print(f"   ‚ö†Ô∏è  HTTP {response.status_code}, th·ª≠ l·∫°i sau {retry_delay} gi√¢y...")
                    time.sleep(retry_delay)
                    continue
                else:
                    print(f"‚ùå L·ªói: HTTP {response.status_code} sau {max_retries} l·∫ßn th·ª≠")
                    return []
            
            break
            
        except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, 
                requests.exceptions.RequestException) as e:
            if attempt < max_retries - 1:
                print(f"   ‚ö†Ô∏è  L·ªói k·∫øt n·ªëi: {str(e)[:50]}...")
                print(f"   ‚ö†Ô∏è  Th·ª≠ l·∫°i l·∫ßn {attempt + 2}/{max_retries} sau {retry_delay} gi√¢y...")
                time.sleep(retry_delay)
            else:
                print(f"‚ùå L·ªói k·∫øt n·ªëi sau {max_retries} l·∫ßn th·ª≠: {str(e)[:100]}")
                return []
        except Exception as e:
            print(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh: {str(e)[:100]}")
            return []
    
    try:
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        import re
        forecast_data = []
        today = datetime.now().date()
        
        date_pattern = re.compile(r'(T[2-7]|CN|Ch·ªß nh·∫≠t|Th·ª© [2-7])\s*\d{1,2}/\d{1,2}', re.IGNORECASE)
        
        found_days = []
        
        all_elements = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'div', 'section', 'article'])
        
        for element in all_elements:
            text = element.get_text()
            
            if date_pattern.search(text):
                temp_matches = re.findall(r'(\d+)\s*¬∞', text)
                main_temp = None
                if temp_matches:
                    for temp in temp_matches:
                        temp_val = int(temp)
                        if 0 <= temp_val <= 50:
                            main_temp = temp
                            break
                
                if main_temp:
                    pressure_matches = re.findall(r'(\d{3,4})\s*hPa', text)
                    wind_matches = re.findall(r'(\d+\.?\d*)\s*km/h', text)
                    rain_matches = re.findall(r'(\d+\.?\d*)\s*mm', text)
                    
                    temp_min_max_pattern = re.findall(r'(\d+\.?\d*)\s*¬∞\s*/\s*(\d+\.?\d*)\s*¬∞', text)
                    temp_min = None
                    temp_max = None
                    if temp_min_max_pattern:
                        temp_min = temp_min_max_pattern[0][0]
                        temp_max = temp_min_max_pattern[0][1]
                    else:
                        temp_min_max_pattern2 = re.findall(r'Th·∫•p[^:]*:\s*(\d+\.?\d*)\s*¬∞[^/]*/\s*Cao[^:]*:\s*(\d+\.?\d*)\s*¬∞', text, re.IGNORECASE)
                        if temp_min_max_pattern2:
                            temp_min = temp_min_max_pattern2[0][0]
                            temp_max = temp_min_max_pattern2[0][1]
                        else:
                            parent = element.find_parent()
                            if parent:
                                parent_text = parent.get_text()
                                temp_min_max_pattern3 = re.findall(r'(\d+\.?\d*)\s*¬∞\s*/\s*(\d+\.?\d*)\s*¬∞', parent_text)
                                if temp_min_max_pattern3:
                                    temp_min = temp_min_max_pattern3[0][0]
                                    temp_max = temp_min_max_pattern3[0][1]
                    
                    day_key = f"{main_temp}_{pressure_matches[0] if pressure_matches else 'none'}"
                    if day_key not in [d.get('key', '') for d in found_days]:
                        found_days.append({
                            'key': day_key,
                            'temp': main_temp,
                            'temp_min': temp_min,
                            'temp_max': temp_max,
                            'pressure': pressure_matches[0] if pressure_matches else None,
                            'wind': wind_matches[0] if wind_matches else None,
                            'rain': rain_matches[0] if rain_matches else None,
                            'text': text[:200]
                        })
        
        found_days = found_days[:1]
        
        print(f"   T√¨m th·∫•y {len(found_days)} ng√†y d·ª± b√°o (ch·ªâ l·∫•y h√¥m nay)")
        
        for idx, day_data in enumerate(found_days):
            forecast_date = today
            
            raw_text = day_data.get('text', '')
            raw_text = raw_text.replace('\n', ' ').replace('\r', ' ').strip()
            raw_text = ' '.join(raw_text.split())
            raw_text = raw_text[:100]
            
            record = {
                'city': CITY_MAPPING.get(city_key, city_key),
                'city_key': city_key,
                'date': forecast_date.strftime('%Y-%m-%d'),
                'source': 'thoitiet360',
                'crawled_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'Temp': parse_temperature(day_data['temp']) if day_data['temp'] else None,
                'Temp_min': parse_temperature(day_data.get('temp_min')) if day_data.get('temp_min') else None,
                'Temp_max': parse_temperature(day_data.get('temp_max')) if day_data.get('temp_max') else None,
                'Pressure': parse_pressure(day_data['pressure']) if day_data['pressure'] else None,
                'Wind': parse_wind(day_data['wind']) if day_data['wind'] else None,
                'Rain': parse_rain(day_data['rain']) if day_data['rain'] else None,
                'Cloud': None,
                'raw_text': raw_text
            }
            
            forecast_data.append(record)
            temp_info = f"Temp={record['Temp']}¬∞C"
            if record['Temp_min'] and record['Temp_max']:
                temp_info += f" (Th·∫•p/Cao: {record['Temp_min']}¬∞C/{record['Temp_max']}¬∞C)"
            print(f"   ‚úì Ng√†y {forecast_date.strftime('%Y-%m-%d')}: {temp_info}, Pressure={record['Pressure']}hPa, Wind={record['Wind']}km/h, Rain={record['Rain']}mm")
        
        if not forecast_data:
            print("   ‚ö†Ô∏è  Kh√¥ng parse ƒë∆∞·ª£c d·ªØ li·ªáu b·∫±ng c√°ch th√¥ng th∆∞·ªùng, th·ª≠ c√°ch kh√°c...")
            print(f"   HTML sample (first 1000 chars): {response.text[:1000]}")
        
        return forecast_data
        
    except Exception as e:
        print(f"‚ùå L·ªói khi parse d·ªØ li·ªáu: {str(e)[:100]}")
        return []

def preprocess_thoitiet360_data(df):
    df = df.copy()
    
    city_mapping = {
        'H√† N·ªôi': 'ha-noi',
        'Vinh': 'vinh',
        'H·ªì Ch√≠ Minh': 'ho-chi-minh-city'
    }
    
    if 'city' in df.columns:
        df['city'] = df['city'].map(city_mapping).fillna(df['city'])
    
    if 'date' in df.columns:
        df['datetime'] = pd.to_datetime(df['date'] + ' 00:00:00')
        df['datetime'] = df['datetime'].dt.strftime('%Y-%m-%d %H:%M:%S')
    
    numeric_cols = ['Temp', 'Temp_min', 'Temp_max', 'Pressure', 'Wind', 'Rain', 'Cloud', 'Gust']
    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        else:
            df[col] = None
    
    columns_to_keep = ['city', 'date', 'datetime', 'Temp', 'Temp_min', 'Temp_max', 'Pressure', 'Wind', 'Rain', 'Cloud', 'Gust']
    columns_to_keep = [col for col in columns_to_keep if col in df.columns]
    df = df[columns_to_keep]
    
    columns_to_drop = ['raw_text', 'city_key', 'source', 'crawled_at', 'Time']
    for col in columns_to_drop:
        if col in df.columns:
            df = df.drop(columns=[col])
    
    if 'city' in df.columns and 'date' in df.columns:
        df = df.sort_values(['city', 'date']).reset_index(drop=True)
    
    return df

def save_to_csv(data, filename='thoitiet360_data.csv'):
    try:
        if not data:
            print("‚ö†Ô∏è  Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ l∆∞u")
            return
        
        df_old = pd.DataFrame()
        import os
        if os.path.exists(filename):
            try:
                df_old = pd.read_csv(filename, encoding='utf-8-sig')
                print(f"üìñ ƒê√£ ƒë·ªçc {len(df_old)} records c≈© t·ª´ {filename}")
            except Exception as e:
                print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ ƒë·ªçc file c≈©: {str(e)}")
                df_old = pd.DataFrame()
        
        df_new = pd.DataFrame(data)
        if 'raw_text' in df_new.columns:
            df_new['raw_text'] = df_new['raw_text'].astype(str).str.replace('\n', ' ').str.replace('\r', ' ').str.strip()
            df_new['raw_text'] = df_new['raw_text'].str[:100]
        
        if not df_old.empty:
            df_combined = pd.concat([df_old, df_new], ignore_index=True)
            df_combined = df_combined.drop_duplicates(subset=['city', 'date'], keep='last')
            df_combined = df_combined.sort_values(['city', 'date']).reset_index(drop=True)
            df_final = df_combined
        else:
            df_final = df_new
        
        df_final.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"‚úÖ ƒê√£ l∆∞u {len(df_final)} records v√†o {filename} (trong ƒë√≥ {len(df_new)} records m·ªõi)")
    except Exception as e:
        print(f"‚ùå L·ªói khi l∆∞u CSV: {str(e)}")

def save_to_database(df):
    try:
        from database import init_database, get_db_connection
        import os
        
        init_database()
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        inserted = 0
        updated = 0
        
        for _, row in df.iterrows():
            cursor.execute('''
                SELECT id FROM thoitiet360_data 
                WHERE city = ? AND date = ?
            ''', (row['city'], row['date']))
            
            existing = cursor.fetchone()
            
            if existing:
                cursor.execute('''
                    UPDATE thoitiet360_data 
                    SET datetime = ?, Temp = ?, Temp_min = ?, Temp_max = ?, Pressure = ?, Wind = ?, Rain = ?, Cloud = ?, Gust = ?
                    WHERE city = ? AND date = ?
                ''', (
                    row.get('datetime'),
                    row.get('Temp'),
                    row.get('Temp_min'),
                    row.get('Temp_max'),
                    row.get('Pressure'),
                    row.get('Wind'),
                    row.get('Rain'),
                    row.get('Cloud'),
                    row.get('Gust'),
                    row['city'],
                    row['date']
                ))
                updated += 1
            else:
                cursor.execute('''
                    INSERT INTO thoitiet360_data 
                    (city, date, datetime, Temp, Temp_min, Temp_max, Pressure, Wind, Rain, Cloud, Gust)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    row['city'],
                    row['date'],
                    row.get('datetime'),
                    row.get('Temp'),
                    row.get('Temp_min'),
                    row.get('Temp_max'),
                    row.get('Pressure'),
                    row.get('Wind'),
                    row.get('Rain'),
                    row.get('Cloud'),
                    row.get('Gust')
                ))
                inserted += 1
        
        conn.commit()
        conn.close()
        
        print(f"‚úÖ ƒê√£ l∆∞u v√†o database: {inserted} records m·ªõi, {updated} records c·∫≠p nh·∫≠t")
        return inserted + updated
        
    except Exception as e:
        print(f"‚ùå L·ªói khi l∆∞u v√†o database: {str(e)}")
        import traceback
        traceback.print_exc()
        return 0

def main():
    check_and_pull_from_github()
    
    print("\n" + "="*70)
    print("CRAWL D·ªÆ LI·ªÜU NG√ÄY H√îM NAY T·ª™ THOITIET360.EDU.VN")
    print("="*70)
    
    all_data = []
    
    cities = ['ha-noi', 'vinh', 'ho-chi-minh']
    
    for idx, city_key in enumerate(cities, 1):
        print(f"\n[{idx}/{len(cities)}] ƒêang crawl {CITY_MAPPING.get(city_key, city_key)}...")
        data = crawl_thoitiet360(city_key)
        all_data.extend(data)
        
        if idx < len(cities):
            time.sleep(3)
    
    print(f"\n{'='*70}")
    print(f"T·ªîNG K·∫æT: Crawl ƒë∆∞·ª£c {len(all_data)} records")
    print(f"{'='*70}")
    
    if all_data:
        df = pd.DataFrame(all_data)
        df_processed = preprocess_thoitiet360_data(df)
        
        save_to_csv(all_data, 'thoitiet360_data.csv')
        
        print("\nüíæ ƒêang l∆∞u v√†o database...")
        save_to_database(df_processed)
        
        print("\nüìä T√≥m t·∫Øt d·ªØ li·ªáu:")
        if not df.empty:
            print(df.groupby('city').size())
            print("\nM·∫´u d·ªØ li·ªáu ƒë√£ l∆∞u:")
            print(df_processed[['city', 'date', 'Temp', 'Pressure', 'Wind', 'Rain']].head(10).to_string())
    else:
        print("‚ö†Ô∏è  Kh√¥ng crawl ƒë∆∞·ª£c d·ªØ li·ªáu n√†o!")

if __name__ == '__main__':
    main()

